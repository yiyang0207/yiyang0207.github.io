---
layout: post
title: "IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models"
date: 2025-03-28 01:38:30 +00:00
image: /images/impact.png
categories: research
authors: "<strong>Yiyang Ling*</strong>, Karan Owalekar*, Oluwatobiloba Adesanya, Erdem Bıyık, Daniel Seita"
venue: "arXiv preprint, 2025"
arxiv: https://arxiv.org/abs/2503.10110
website: https://impact-planning.github.io/
# code: https://github.com/liruiw/GenSim
---
We introduce IMPACT, a novel motion planning framework that uses Vision-Language Models to infer environment semantics, identifying which parts of the environment can best tolerate contact based on object properties and locations. Results both in simulation and real-world suggest that IMPACT enables robots to efficiently reach targets while making semantically acceptable contact when needed.